{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before you start: \n",
        "1. Make sure you have cudatoolkit and cudnn in your environment\n",
        "2. Make sure your environment has CUDA supported torch, torchvision torchaudio from [link](https://pytorch.org/get-started/locally/) before doing pip install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwKvwFgJjKcd",
        "outputId": "157b792d-ebdd-47ee-c3d1-6cab30e17d27"
      },
      "outputs": [],
      "source": [
        "# !pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "notebook login is required to download the model, you can also try other models that dont require login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QpNAhJOoi4Tc",
        "outputId": "438563f0-5956-4c24-d3a7-05dfc8584998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: HF_HUB_ENABLE_HF_TRANSFER=True\n"
          ]
        }
      ],
      "source": [
        "# download and upload weights faster\n",
        "%env HF_HUB_ENABLE_HF_TRANSFER=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Oct 11 11:20:27 2024       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.76.01              Driver Version: 552.22         CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 4070 Ti     On  |   00000000:01:00.0  On |                  N/A |\n",
            "|  0%   39C    P8              3W /  285W |     518MiB /  12282MiB |      7%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A        36      G   /Xwayland                                   N/A      |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# check if GPU is available\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check if pytorch is using GPU\n",
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZkkyTnQLi4Td"
      },
      "outputs": [],
      "source": [
        "cache_dir=\"\"\n",
        "\n",
        "# model_id = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
        "model_id = \"mistralai/Mistral-7B-v0.1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "63TXZOXsi4Te",
        "outputId": "88b4daf7-973b-4725-d81c-009d3364ac80"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a100c1d0db14d31911deb828b036a91",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
        "\n",
        "# quantization using BnB\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\", # Normalized Float 4\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# loading the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    # attn_implementation=\"flash_attention_2\", # FlashAttention only supports Ampere GPUs or newer.\n",
        "    cache_dir=cache_dir\n",
        ")\n",
        "\n",
        "# loading the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NxkRx0CVi4Tf"
      },
      "outputs": [],
      "source": [
        "# check if any weights/bias overflowed onto cpu (meta)\n",
        "for n, p in  model.named_parameters():\n",
        "    if p.device.type == \"meta\":\n",
        "        print*(f\"{n} is on meta!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7I7Kooi52fyc"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable() # save some VRAM\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"transformer_1.webp\" alt=\"description\" width=\"300\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mbZiADcT21iJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MistralForCausalLM(\n",
            "  (model): MistralModel(\n",
            "    (embed_tokens): Embedding(32000, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x MistralDecoderLayer(\n",
            "        (self_attn): MistralSdpaAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): MistralRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): MistralMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "J6sjPWvi3MHT"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=4,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\n",
        "        # \"q_proj\",\n",
        "        # \"k_proj\",\n",
        "        # \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        # \"self_attn.rotary_emb.inv_freq\",\n",
        "        # \"gate_proj\",\n",
        "        # \"up_proj\",\n",
        "        # \"down_proj\",\n",
        "        # \"lm_head\",\n",
        "        # \"input_layernorm.weight\",\n",
        "        # \"post_attention_layernorm.weight\",\n",
        "        # \"model.norm.weight\",\n",
        "        # \"lm_head.weight\",\n",
        "    ],\n",
        "    layers_to_transform = [15],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\", # Bias can be 'none', 'all' or 'lora_only'.\n",
        "    task_type=\"CASUAL_LM\",\n",
        "    # use_dora=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TwkDigXt383R"
      },
      "outputs": [],
      "source": [
        "model = get_peft_model(model, peft_config) # move to peft model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- you will get 10,630,144 trainable parameters if using all modules and layers.\n",
        "- thats still a 1:700 ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vWuDNKTh4Bl3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 32,768 || all params: 7,241,764,864 || trainable%: 0.0004524863844018894\n"
          ]
        }
      ],
      "source": [
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCtVWpQu4gGs"
      },
      "source": [
        "# Setup Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZdYy4Beu5KBW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.chat_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x8ta37nX41uc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[INST] write a quick sort algorithm in python [/INST] here you are. </s><s>[INST] great. [/INST]\n"
          ]
        }
      ],
      "source": [
        "# Copied from https://discuss.huggingface.co/t/issue-with-llama-2-chat-template-and-out-of-date-documentation/61645/3\n",
        "tokenizer.chat_template = (\n",
        "    \"{% if messages[0]['role'] == 'system' %}\"\n",
        "    \"{% set loop_messages = messages[1:] %}\"  # Extract system message if it's present\n",
        "    \"{% set system_message = messages[0]['content'] %}\"\n",
        "    \"{% elif USE_DEFAULT_PROMPT == true and not '<<SYS>>' in messages[0]['content'] %}\"\n",
        "    \"{% set loop_messages = messages %}\"  # Or use the default system message if the flag is set\n",
        "    \"{% set system_message = 'DEFAULT_SYSTEM_MESSAGE' %}\"\n",
        "    \"{% else %}\"\n",
        "    \"{% set loop_messages = messages %}\"\n",
        "    \"{% set system_message = false %}\"\n",
        "    \"{% endif %}\"\n",
        "    \"{% if loop_messages|length == 0 and system_message %}\"  # Special handling when only sys message present\n",
        "    \"{{ bos_token + '[INST] <<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n [/INST]' }}\"\n",
        "    \"{% endif %}\"\n",
        "    \"{% for message in loop_messages %}\"  # Loop over all non-system messages\n",
        "    \"{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\"\n",
        "    \"{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\"\n",
        "    \"{% endif %}\"\n",
        "    \"{% if loop.index0 == 0 and system_message != false %}\"  # Embed system message in first message\n",
        "    \"{% set content = '<<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n' + message['content'] %}\"\n",
        "    \"{% else %}\"\n",
        "    \"{% set content = message['content'] %}\"\n",
        "    \"{% endif %}\"\n",
        "    \"{% if message['role'] == 'user' %}\"  # After all of that, handle messages/roles in a fairly normal way\n",
        "    \"{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}\"\n",
        "    \"{% elif message['role'] == 'system' %}\"\n",
        "    \"{{ '<<SYS>>\\\\n' + content.strip() + '\\\\n<</SYS>>\\\\n\\\\n' }}\"\n",
        "    \"{% elif message['role'] == 'assistant' %}\"\n",
        "    \"{{ ' '  + content.strip() + ' ' + eos_token }}\"\n",
        "    \"{% endif %}\"\n",
        "    \"{% endfor %}\"\n",
        ")\n",
        "\n",
        "messages=[\n",
        "    {\"role\": \"user\", \"content\": \"write a quick sort algorithm in python\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"here you are.\"},\n",
        "    {\"role\": \"user\", \"content\": \"great.\"},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4KAycmqA5r-d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.special_tokens_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.add_special_tokens({'pad_token': '<|PAD|>'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'bos_token': '<s>',\n",
              " 'eos_token': '</s>',\n",
              " 'unk_token': '<unk>',\n",
              " 'pad_token': '<|PAD|>'}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.special_tokens_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kvoOYKeH5-78"
      },
      "outputs": [],
      "source": [
        "# trainable_params_names = [\n",
        "#     \"embed_tokens\",\n",
        "#     \"input_layernorm\",\n",
        "#     \"post_attention_layernorm\",\n",
        "# ] # for llama models\n",
        "\n",
        "# for name, parameter in model.named_parameters():\n",
        "#   if any(trainable_param in name for trainable_param in trainable_params_names):\n",
        "#     parameter.requires_grad_(True)\n",
        "#   else:\n",
        "#     parameter.requires_grad_(False)\n",
        "\n",
        "# trainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n",
        "\n",
        "# trainable_params_state_dict = {n: p.data for n, p in trainable_params.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6t2fAhge9yii"
      },
      "outputs": [],
      "source": [
        "# trainable_params.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32000\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wcpHvxBzi4Ti"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Embedding(32016, 4096)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5a2qGu72i4Tj"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import json\n",
        "\n",
        "# https://huggingface.co/datasets/ksuriuri/AI_Vtuber_Chat_History\n",
        "dataset=\"ksuriuri/AI_Vtuber_Chat_History\"\n",
        "\n",
        "# data = load_dataset(dataset)\n",
        "\n",
        "data = json.load(open(\"AI_vtuber.json\", encoding=\"utf-8\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'prompt': '什么样的裙子呢', 'response': '啊，是一条纯白色的连衣裙。它上面印满了小猫图案，好可爱呢！我穿着它感觉像一个小公主。嘿嘿。', 'history': [['你好呀', '嘿嘿，你好呀！今天我穿了一件新裙子，感觉超级棒呢！']]}\n"
          ]
        }
      ],
      "source": [
        "print(data[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'prompt': 'What kind of dress?', 'response': 'Ah, it's a plain white dress. It has kitten prints all over it and it's so cute! I feel like a little princess in it. Hehehe.' , 'history': [['Hello there', 'Hey there! I'm wearing a new dress today, and I feel super great!']]}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import deepl\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# 500,000 character limit / month\n",
        "translator = deepl.Translator(auth_key=os.getenv(\"DEEPL_API_KEY\"))\n",
        "\n",
        "result = translator.translate_text(str(data[1]), target_lang=\"EN-GB\")\n",
        "print(result.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Mj1YtUtuMSMR"
      },
      "outputs": [],
      "source": [
        "new_data = []\n",
        "for message in data:\n",
        "    messages = []\n",
        "    if message[\"history\"]:\n",
        "        for history in message[\"history\"]:\n",
        "            message_format = [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": translator.translate_text(history[0], target_lang=\"EN-GB\").text\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": translator.translate_text(history[1], target_lang=\"EN-GB\").text\n",
        "                }\n",
        "            ]\n",
        "            messages.extend(message_format)\n",
        "    message_format = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": translator.translate_text(message[\"prompt\"], target_lang=\"EN-GB\").text\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": translator.translate_text(message[\"response\"], target_lang=\"EN-GB\").text\n",
        "        }\n",
        "    ]\n",
        "    messages.extend(message_format)\n",
        "    new_data.append(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[{'role': 'user', 'content': 'Hello.'}, {'role': 'assistant', 'content': \"Hey, hey, how are you? I'm wearing a new dress today, and I feel super great about it!\"}], [{'role': 'user', 'content': 'Hello.'}, {'role': 'assistant', 'content': \"Hey, hey, how are you? I'm wearing a new dress today, and I feel super great about it!\"}, {'role': 'user', 'content': 'What kind of dress?'}, {'role': 'assistant', 'content': \"Ah, it's a plain white dress. It had kitten prints all over it and it was so cute! I feel like a little princess in it. Heh heh.\"}], [{'role': 'user', 'content': 'Hello, Yuri.'}, {'role': 'assistant', 'content': \"Hey, miss Greenie, you're awake.\"}], [{'role': 'user', 'content': 'Hello, Yuri.'}, {'role': 'assistant', 'content': \"Hey, miss Greenie, you're awake.\"}, {'role': 'user', 'content': \"What's your name?\"}, {'role': 'assistant', 'content': \"Hey, I'm Yuri. You can call me Yuri.\"}], [{'role': 'user', 'content': 'Hello, Yuri.'}, {'role': 'assistant', 'content': \"Hey, miss Greenie, you're awake.\"}, {'role': 'user', 'content': \"What's your name?\"}, {'role': 'assistant', 'content': \"Hey, I'm Yuri. You can call me Yuri.\"}, {'role': 'user', 'content': 'What do you like?'}, {'role': 'assistant', 'content': \"Well, I'm super in love with the blue eyes of the humans, and their music, and the blue skies and the fresh cherry blossoms, I love love love it!\"}]]\n"
          ]
        }
      ],
      "source": [
        "print(new_data[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('translated_AI_vtuber.json', 'w', encoding=\"utf-8\") as json_file:\n",
        "    json.dump(new_data, json_file, indent=4, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs = []\n",
        "for row in new_data:\n",
        "    new_dict = {}\n",
        "    tokenized_chat = tokenizer.apply_chat_template(row, tokenize=False, add_generation_prompt=True)\n",
        "    new_dict[\"text\"] = tokenized_chat\n",
        "    inputs.append(new_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s><s>[INST] Hello. [/INST] Hey, hey, how are you? I'm wearing a new dress today, and I feel super great about it! </s>\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "tokens = tokenizer.encode(inputs[0][\"text\"], add_special_tokens=True)\n",
        "print(tokenizer.decode(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"AI_Vtuber_Llama\"\n",
        "dataset_name = \"AI_Vtuber\"\n",
        "\n",
        "epochs=1\n",
        "batch_size=1\n",
        "grad_accum=8\n",
        "context_length=512*8\n",
        "save_dir = f\"models/{model_name}_{dataset_name}_{epochs}_{batch_size}_lora\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('translated_AI_vtuber_templated.json', 'w', encoding=\"utf-8\") as json_file:\n",
        "    json.dump(inputs, json_file, indent=4, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e8d16b67a5b467a88f16592bef1f679",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = load_dataset(\"json\", data_files=\"translated_AI_vtuber_templated.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 36\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3a76ca7cda543a19a828d2b7af6025a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/joey/miniconda3/envs/lora/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n",
            "/home/joey/miniconda3/envs/lora/lib/python3.10/site-packages/accelerate/accelerator.py:463: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "Using auto half precision backend\n",
            "/home/joey/miniconda3/envs/lora/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:324: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "# from transformers import Trainer\n",
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=context_length,\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    packing=True,\n",
        "    args=transformers.TrainingArguments(\n",
        "        max_steps=1,\n",
        "        save_steps=1,\n",
        "        num_train_epochs=epochs,\n",
        "        output_dir=save_dir,\n",
        "        gradient_accumulation_steps=grad_accum,\n",
        "        # evaluation_strategy=\"steps\",\n",
        "        # do_eval=True,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        fp16=True,\n",
        "        # bf16=True, # fp16=True for non-ampere GPUs\n",
        "        optim=\"adamw_torch\",\n",
        "        learning_rate=1e-4,\n",
        "        log_level=\"debug\",\n",
        "        remove_unused_columns=False\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Currently training with a batch size of: 1\n",
            "***** Running training *****\n",
            "  Num examples = 1\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 1\n",
            "  Number of trainable parameters = 32,768\n",
            "/home/joey/miniconda3/envs/lora/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/home/joey/miniconda3/envs/lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 00:02, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to models/AI_Vtuber_Llama_AI_Vtuber_1_1_lora/checkpoint-1\n",
            "loading configuration file config.json from cache at /home/joey/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/7231864981174d9bee8c7687c24c8344414eae6b/config.json\n",
            "Model config MistralConfig {\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.45.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "/home/joey/miniconda3/envs/lora/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "tokenizer config file saved in models/AI_Vtuber_Llama_AI_Vtuber_1_1_lora/checkpoint-1/tokenizer_config.json\n",
            "Special tokens file saved in models/AI_Vtuber_Llama_AI_Vtuber_1_1_lora/checkpoint-1/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1, training_loss=0.2495785802602768, metrics={'train_runtime': 8.6984, 'train_samples_per_second': 0.92, 'train_steps_per_second': 0.115, 'total_flos': 174753998438400.0, 'train_loss': 0.2495785802602768, 'epoch': 1.0})"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.config.use_cache = False\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import gc # garbage collection\n",
        "\n",
        "def stream(user_prompt, base_model, tokenizer, checkpoint=\"\"):\n",
        "\n",
        "    if base_model:\n",
        "        eval_model = model\n",
        "    else:\n",
        "        eval_model = PeftModel.from_pretrained(model, checkpoint)\n",
        "        eval_model = eval_model.to(\"cuda\")\n",
        "\n",
        "    eval_model.config.use_cache = True\n",
        "\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": f\"{user_prompt.strip()}\"},\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    inputs = tokenizer([inputs], return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
        "\n",
        "    if \"token_type_ids\" in inputs:\n",
        "        del inputs[\"token_type_ids\"]\n",
        "\n",
        "    streamer = TextStreamer(tokenizer)\n",
        "\n",
        "    print(f\"eval_model is on: {next(eval_model.parameters()).device}\")\n",
        "    print(f\"inputs is on: {inputs['input_ids'].device}\")\n",
        "\n",
        "    _ = eval_model.generate(**inputs, streamer=streamer, max_new_tokens=100, do_sample=False, temperature=1.0, top_k=50)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def evaluate(base_model, tokenizer, checkpoint=\"\"):\n",
        "    questions = [\n",
        "        \"What are you doing?\",\n",
        "        \"Do you prefer coffee or tea?\",\n",
        "    ]\n",
        "\n",
        "    answers = [\n",
        "        \"...\",\n",
        "        \"...\",\n",
        "    ]\n",
        "\n",
        "    for question, answer in zip(questions, answers):\n",
        "        stream(question, base_model=base_model, tokenizer=tokenizer, checkpoint=checkpoint)\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval_model is on: cuda:0\n",
            "inputs is on: cuda:0\n",
            "<s>[INST] What are you doing? "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/home/joey/miniconda3/envs/lora/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/home/joey/miniconda3/envs/lora/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[/INST]\n",
            "\n",
            ",,,,,,,,,,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "eval_model is on: cuda:0\n",
            "inputs is on: cuda:0\n",
            "<s>[INST] Do you prefer coffee or tea? [/INST]\n",
            "\n",
            "\n",
            "\n",
            "[INST] [/INST]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[INST] [/INST]\n",
            "\n",
            "[INST] [/INST]\n",
            "\n",
            "[INST] [/INST]\n",
            "\n",
            "[INST] Do you prefer [/INST]\n",
            "\n",
            "\n",
            "[INST] [/INST]\n",
            "\n",
            "\n",
            "[INST] Do [/INST]\n",
            "\n",
            "[INST] Do\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate(base_model=True, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModel(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32016, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-14): 15 x MistralDecoderLayer(\n",
              "            (self_attn): MistralSdpaAttention(\n",
              "              (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "              (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              (rotary_emb): MistralRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "          (15): MistralDecoderLayer(\n",
              "            (self_attn): MistralSdpaAttention(\n",
              "              (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "              (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (rotary_emb): MistralRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "          (16-31): 16 x MistralDecoderLayer(\n",
              "            (self_attn): MistralSdpaAttention(\n",
              "              (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "              (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              (rotary_emb): MistralRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32016, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.config.use_cache = True\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models/AI_Vtuber_Llama_AI_Vtuber_1_1_lora\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval_model is on: cuda:0\n",
            "inputs is on: cuda:0\n",
            "<s>[INST] What are you doing? [/INST]\n",
            "\n",
            ",,,,,,,,,,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,in,\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval_model is on: cuda:0\n",
            "inputs is on: cuda:0\n",
            "<s>[INST] Do you prefer coffee or tea? [/INST]\n",
            "\n",
            "\n",
            "\n",
            "[INST] [/INST]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[INST] [/INST]\n",
            "\n",
            "[INST] [/INST]\n",
            "\n",
            "[INST] [/INST]\n",
            "\n",
            "[INST] Do you prefer [/INST]\n",
            "\n",
            "\n",
            "[INST] [/INST]\n",
            "\n",
            "\n",
            "[INST] Do [/INST]\n",
            "\n",
            "[INST] Do\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(save_dir)\n",
        "evaluate(base_model=False, tokenizer=tokenizer, checkpoint=f\"{save_dir}/checkpoint-1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lora",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
